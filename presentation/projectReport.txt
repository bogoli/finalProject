Final Project
CS 2420
Lia Bogoev & John Call

Introduction

Algorithms
	The three main factors that influence the speed of our engine are the number of words in documents, the number of documents, and the number of words in the query. The size and quantity of documents affects the running time through the indexing function. The data structure that we chose to use was a dictionary (basically a hash table). We used a dictionary for each document, to track the number of occurences of each word in a document. We also used a dictionary of arrays, which contained all the words in all the documents (excluding stop words) as the keys, and an array of the documents which contained each word as the values. Filling these arrays is roughly O(n), based on the quantity and size of the documents. 
	
	The algorithm that we use to address the boolean operators in the search query (parseQuery) is recursive. This gives it a running time of O(2^n), where n is the number of words in the query. Given that the majority of queries will not be more than five words, recursion is a fine choice. The code itself is clean and readable and functions well. 

	We also implemented document ranking. Instead of writing our own quickSort or mergeSort algorithm to implement document ranking, we used Python’s built in timsort, because its time complexity in best case is better than both quickSort and mergeSort, O(n). Average case and worst case are both the equivalent of mergeSort, O(n*log(n)). Timsort makes use of already sorted subsets of data, storing them in temporary arrays before merging them. The only potential hazard with timsort is space complexity, which is the worst case is O(n). With such a small scale project, however, space was not of much concern. 
	
Design Issues
	At first glance, handling the boolean operators of the search queries seemed trivial. Once we began to write the function, however, it quickly became apparent that we would need to either limit the number of search terms so that we could focus on one boolean operator at a time, or we would need to write a recursive function. We chose to go with recursion so that we could have flexibility in the number of search items. This required a restructuring of the existing parsing algorithm so that it could accomodate recursion. 

	The way that we designed it, the parseQuery function initially takes the dictionary of all the words from the indexing function, and uses the AND and OR operators in the query itself to cut and merge dictionaries, returning a dictionary of the documents that contain the search terms in the query. 
	
Optimization
	With regards to optimization, not much can be done to improve the indexing. In any situation, all of the words in a document and all of the documents themselves have to be indexed. Optimizing indexing is just a matter of throwing more machines and more power at the problem. 

	Within the searching part of the algorithm, however, some optimization could perhaps be achieved with dividing and conquering the search query. The parseQuery function has a worst case of ~O(n), if the query is a common word that is present in all of the documents. If the algorithm could be broken up in such a way that common words are treated differently than rare words, then the search time for common words would be improved. 

Scalability
	if a lot of documents, index things in parallel (just like google)
	many results would need pagination 

	We have three main issues with scalability. One issues lies in our indexing function, the other two deal mainly with the implementation of our interface. Because our indexing function is interative, with a large quantity of documents, or with very large documents, our program would start to run very slowly. Ideally, the way this would be avoided is with multiple machines indexing documents at the same time, in the same way that Google indexes the web. 

	Also, the way that our interface is implemented, any time a query is submitted, the entire python program is called again, which calls the indexing function. So in essence, every query re-indexes all of the documents. This isn't an issue with our project, because the size and quantity of documents is smalls. Ideally, however, the indexed hash tables would be stored when the page loads, and then the search algorithm can be virtually instantaenous — O(1).

	The other issue that arises with our interface implementation is the need for pagination. As it is, our interface displays all of the relevant documents. However, if there were many documents, splitting them into groups of ten per page would probably be ideal. 

Sample Query Results
	Screenshots --> in a real word processing document

Graphic Design Notes
	The face of any programming project inherently influences how people react to it. Aside from its incredibly complex and groundbreaking algorithms, Google’s simple, minimalist design draws people in. Inspired by Google’s originality, we implemented a clean HTML interface to showcase our search engine. We felt that showing the search query in context with highlighting would enhance the usefulness of our engine, and that the design as a whole elevates the experience of searching. 