Final Project
CS 2420
Lia Bogoev & John Call

Introduction

Algorithms
	The three main factors that influence the speed of our engine are the number of words in documents (W), the number of documents (D), and the number of words in the query (Q). The size and quantity of documents affects the running time due to the process of reading each file for indexing. The primary data structure that we use is a dictionary (basically a hash table). We use a dictionary for each document, to track the number of occurrences of each word in the given document. We also use a dictionary of arrays, which contain all words in all the documents (excluding stop words) and an array of documents which contain said word as key value pairs. Filling these arrays is O(n), based on the quantity and size of the documents.

	The algorithm that we use to address the boolean operators in the search query (parseQuery) is recursive. Because it must iterate over each word in each document for each part of the search query, in the worst case, it is O(Q * W * D^2). Given that the majority of queries will not be very complicated, this is approximately O(W * D ^ 2), which is not the fastest, but it's not noticeable until the number of documents or the diversity of words within the various documents increases. The code itself is clean, readable and functions well. In larger scales, a more robust method involving a database engine, perhaps such as MySQL, would clearly be desirable.

	We also implement document ranking. The algorithm considers two factors: Word Frequency (how often a word from the query appears in the document), and Document Frequency (how often a word appears in all documents). In principle, if a word from the search query appears in the document more times, then it is more likely to be the document the user is searching for. Conversely if the same word exists in many documents, then it is less specific to a given document and, therefore, it is less likely that particular word from the search query has an influence on what the user is searching for. The time to look up both of these factors from the index is O(1) because of our use of a hash table, making this portion of the algorithm very nice, indeed.

	Instead of writing our own sorting method, ie quickSort or mergeSort, to sort the documents by ranking, we used Python’s built in timsort, because its time complexity, in best case, is better than both quickSort and mergeSort, O(n). Average case and worst case are both the equivalent of mergeSort, O(n*log(n)). Timsort makes use of already sorted subsets of data, storing them in temporary arrays before merging them. The only potential hazard with timsort is space complexity, which is the worst case is O(n). With such a small scale project, however, space was not of much concern.

Design Issues
	At first glance, handling the boolean operators of the search queries seemed trivial. Once we began to write the function, however, it quickly became apparent that we would need to either limit the number of search terms so that we could focus on one boolean operator at a time, or we would need to write a recursive function. We chose to go with recursion so that we could have flexibility in the number of search items. This required a restructuring of the existing parsing algorithm so that it could accommodate recursion.

	Our parseQuery function initially takes the dictionary of all the words from the indexing function, and uses the AND and OR operators in the query itself to cut and merge dictionaries, returning a dictionary of the documents that contain the search terms in the query. It's a pretty rudimentary implementation of Set operations.

Optimization and Scalability
	With regards to optimization, not much can be done to improve the indexing. In any situation, all of the words in a document and all of the documents themselves have to be indexed. However, as we've discussed in class, portions of the document could be indexed in parallel across multiple computers to reduce the time required.

	Within the searching part of the algorithm, however, some optimization could be achieved with dividing and conquering the search query. The parseQuery function has a worst case of ~O(W * D ^ 2), if the query is comprised of common words that are present in many documents. If the algorithm could be broken up in such a way that common words are treated differently than specific words, then the overall time complexity could be reduced.

	There are some things that need to be addressed due to the nature of web browsers and their interaction with the server. Our website asynchronously loads content based on the results of a given query. As the results get larger, the resources and time required by the browser to dynamically generate and insert that HTML content is significant. The first optimization to be performed, therefore, would be to implement a pagination feature. Similar to how Google only shows a limited number of search results on the "first page" and results after that are unlikely to be what you're looking for regardless. (insert http://xkcd.com/1334/)

	The second optimization that needs to occur based on the nature of a webserver is that the indexing loads with each query, after which memory is released back to the operating system once this script is finished executing. In an ideal scenario our indexer would remain active as long as the server is on, or better yet store the index to the file system for long term usage, and wait for requests from the web server. This requires the use of sockets, or other methods, that we do not yet have much practice in implementing.

Sample Query Results
	Screenshots --> in a real word processing document

Graphic Design Notes
	The face of any programming project inherently influences how people react to it. Aside from its incredibly complex and groundbreaking algorithms, Google’s simple, minimalist design draws people in. Inspired by Google’s originality, we implemented a clean HTML interface to showcase our search engine. We felt that showing the search query in context with highlighting would enhance the usefulness of our engine, and that the design as a whole elevates the experience of searching.
